---
layout: article
title: Machine Learning on `dask`
date: 2019-06-19 15:52:00
published: true
categories: [distributed-computing,machine-learning,dask]
comments: false
share: true
description: Running custom machine learning workloads on a dask cluster.
usemathjax: false
author: matti_lyra
---

I've used `dask` for ever since I first heard about it in a PyData Berlin Keynote talk [LINK] by Matthew Rocklin, the
core developer. I found the `dask.bag` especially useful as I was and still am doing quite a lot of `json` formatted
log parsing.

When I joined Comtravo I was really glad to find out that `dask` is also used here, although in a slightly different
context. The data science team at Comtravo uses `dask` to coordinate fairly complex machine learning workloads. Our NLP
pipeline has a lot of cross dependencies between the different predictive models; it's really useful to have an easy,
lightweight way to encode and execute model dependencies.


# What is `dask`?


# Dependency Graphs 101

The foundational layer of `dask` is the computational graph, which encodes work to be done (executable functions) and
the dependencies those pieces or work depend on. Computational graphs are not specific to `dask`, they come up in all
kinds of situtations from parsing formal languages and deep learning to dependency resolution in package managers.

Formally computational graphs are a part of $\lambda$-calculus (lambda-calculus), which are a form of function
abstraction. $\lambda$-calculus is a universal model of computation, so although many of the examples below look simple
the paradigm they follow is very powerful.

Let's see how a simple dependency graph is encoded in `dask`. Since `dask` is pure python the graph is just encoded as
python `dict`. The example below is taken from the `dask` documentation

```python
def inc(i):
    return i+1

def add(a, b):
    return a + b

dsk = {'x': 1,
       'y': (inc, 'x',
       'z': (add, 'x', 'y')}
```

Let's unpack what is happening in the code above. There graph has three nodes named `x`, `y`, `z`. The `x` node just
contains data, in this case the integer `1`. Node `y` is an executable function `inc` which takes one parameter. The
parameter in the graph for that executable function is `x`, or rather the output from executing the node named `x`, but
since `x` just contains the integer `1` that'll be what gets passed in to `y` when it is called.

The node named `z` is where interesting things start to happen, it's also an executable function and takes two
parameters: `x` and `y`. The `x` we're already familiar with; the value of node `y` on the other hand is an executable
so `dask` will call the function and pass the output of that function call as the second parameter to `z`. This simple
chaining of nodes allows us to encode very complex dependencies. So let's get back to machine learning.

# Machine Learning pipelines on `dask`.




# Execute where ever

# A few tricks to ease the pain
